{
  "trainingData": {
    "name": "MNIST",
    "tag": "training-data",
    "html": {
      "EN": [
        "<img src='/tensorflow.js-classification-example/mnist/mnist.png' />",
        "",
        "<a href='http://yann.lecun.com/exdb/mnist/'>MNIST database</a><i>(Modified National Institute of Standards and Technology database)</i>is the base of the numeric cursive image data.",
        "The image is 28x28 size of gray scale consisting of 60000 learning data and 10000 test data. There are 10 classess from 0 to 9.",
        "It is easy to classify, and the capacity of the required models are small. And there are big-scale benchmarks, which make it a good dataset for first-time machine-learning users.",
        "Deep network architecture + augmentation + use ensembles ,you can get more than 99.79% accuracy. <i>(<a href='https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354'>MNIST - who is the best in MNIST</a>)</i>"
      ],
      "KO": [
        "<img src='/tensorflow.js-classification-example/mnist/mnist.png' />",
        "",
        "<a href='http://yann.lecun.com/exdb/mnist/'>MNIST database</a><i>(Modified National Institute of Standards and Technology database)</i>는 숫자 필기체 이미지로 구성된 database이다.",
        "이미지는 28 x 28 사이즈의 gray scale이며 학습 데이터 60000장, 테스트 데이터 10000장으로 구성 되어있다. 클래스는 0 부터 9까지 총 10개 이다.",
        "분류 난이도가 쉬운 편이고 필요한 모델의 용량도 작은 편이며 오랫동안 축척된 벤치마크가 있어서 처음 머신러닝을 접하는 사람들이 이용하기 좋은 데이터셋이다.",
        "깊은 네트워크 구조 + augmentation + 앙상블들을 이용하면 99.79% 이상의 정확도 까지 얻을 수 있다.<i>(<a href='https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354'>MNIST - who is the best in MNIST</a>)</i>"
      ]
    }
  },
  "archetecture": {
    "name": "Custom-CNN",
    "tag": "network archetecture",
    "data": {
      "table": {
        "head": [
          "Layer (type)",
          "Output Shape",
          "Param #"
        ],
        "body": [
          [
            "0 (InputLayer)",
            "[(None, 28, 28, 1)]",
            "0"
          ],
          [
            "1 (Conv2D)",
            "(None, 28, 28, 16)",
            "160"
          ],
          [
            "2 (MaxPooling2D)",
            "(None, 14, 14, 16)",
            "0"
          ],
          [
            "3 (Conv2D)",
            "(None, 14, 14, 32)",
            "4640"
          ],
          [
            "4 (MaxPooling2D)",
            "(None, 7, 7, 32)",
            "0"
          ],
          [
            "5 (Flatten)",
            "(None, 1568)",
            "0"
          ],
          [
            "6 (Dense)",
            "(None, 40)",
            "62760"
          ],
          [
            "7 (Dense)",
            "(None, 10)",
            "410"
          ]
        ]
      },
      "comment": {
        "Total params": "67,970",
        "Trainable params": "67,970",
        "Non-trainable params": "0"
      }
    },
    "html": {
      "EN": [
        "The simple network used consists of Convolution layers<a class='super' href='#footnote-convolution-layer'>[3]</a>, Fully connected layers<a class='super' href='#footnote-fully-connected-layer'>[4]</a>and MaxPooling<a class='super' href='#footnote-max-pooling'>[5]</a>",
        "activation functions of all layers are ReLU<a class='super' href='#footnote-relu'>[6]</a>."
      ],
      "KO": [
        "Convolution Layer<a class='super' href='#footnote-convolution-layer'>[3]</a>, Fully connected layer<a class='super' href='#footnote-fully-connected-layer'>[4]</a>와 MaxPooling<a class='super' href='#footnote-max-pooling'>[5]</a>으로 이루어진 간단한 Convolutional neural network이다.",
        "activation Function은 모든 레이어에 ReLU<a class='super' href='#footnote-relu'>[6]</a>를 사용하였다."
      ]
    }
  },
  "training": {
    "name": "Training",
    "tag": "tuning & learning",
    "html": {
      "EN": [
        "training python script can be find in <a href='https://github.com/whwnsdlr1/tensorflow.js-classification-example/blob/master/train/mnist/train.py'>train.py</a>",
        "It has divided again training data into 50000 images of real training data and 10000 images of verification data.",
        "trained with the following hyperparameters and stored the model with the lowest loss for the validation data.",
        "&nbsp;&nbsp;optimizer: adam<a class='super' href='#footnote-adam'>[8]</a>",
        "&nbsp;&nbsp;loss: cross-entropy<a class='super' href='#footnote-cross-entropy'>[7]</a>",
        "&nbsp;&nbsp;learning-rate: 0.001",
        "&nbsp;&nbsp;batch-size: 32",
        "&nbsp;&nbsp;epochs: 100 // but with early stoppping, train process stopped at epoch-17",
        "i7-8700 CPU / tensorflow 2.0 were used to train and took no more than three minutes.",
        "The accuracy for the training data of the final model was 0.9979, the verification data was 0.9886 and the test data was 0.9907."
      ],
      "KO": [
        "학습 코드는 <a href='https://github.com/whwnsdlr1/tensorflow.js-classification-example/blob/master/train/mnist/train.py'>train.py</a>에서 찾을 수 있다.",
        "60000장의 학습 데이터를 다시 50000장과 10000장의 학습, 검증 데이터로 나눴다.",
        "아래의 하이퍼파리미터로 학습 하였고 검증 데이터에 대해서 가장 낮은 loss를 가지는 epoch 시점의 모델을 저장하였다.",
        "&nbsp;&nbsp;optimizer: adam<a class='super' href='#footnote-adam'>[8]</a>",
        "&nbsp;&nbsp;loss: cross-entropy<a class='super' href='#footnote-cross-entropy'>[7]</a>",
        "&nbsp;&nbsp;learning-rate: 0.001",
        "&nbsp;&nbsp;batch-size: 32",
        "&nbsp;&nbsp;epochs: 100 // but with early stoppping, train process stopped at epoch-17",
        "학습에는 i7-8700 CPU / tensorflow 2.0이 사용하였고 3분에 채 안걸렸다.",
        "최종 모델의 학습 / 검증 / 테스트 accuracy는 0.9979 / 0.9886 / 0.9907였다."
      ]
    }
  },
  "footnote": ["Softmax function", "Grad-CAM", "Convolution layer", "Fully connected layer", "Max pooling", "ReLU", "Cross entropy", "Adam"]
}